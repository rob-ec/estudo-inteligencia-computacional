{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acb5761-5ed3-42ed-825f-a10edb47bdc6",
   "metadata": {},
   "source": [
    "# I.C.: Módelos de Árvores e Ensembles\n",
    "\n",
    "Robson Mesquita Gomes  \n",
    "[<robson.mesquita56@gmail>](mailto:robson.mesquita56@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e5e7b-941b-429e-83c6-90ab542dc957",
   "metadata": {},
   "source": [
    "## Teoria da Informação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035cf7b-5083-4eca-a623-37e7b1d6989a",
   "metadata": {},
   "source": [
    "### Informação\n",
    "\n",
    "$I$ é uma função que mensura a quantidade de informação gerada pela probabiliade $P(x)$ para cada valor $x \\in X$\n",
    "\n",
    "$$I(x) = -\\log{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecef83-66e7-4b74-9843-a1b20424ac6b",
   "metadata": {},
   "source": [
    "### Entropia\n",
    "\n",
    "A entropia é a média da informação em uma variável, ou seja, a soma das informações multiplicadas pelas probabilidades. É uma medida de imprevisibiliade, ou seja, da desordam da distribuição de probabilidade $P$.\n",
    "\n",
    "$$H(X) = \\mathbb{E}[ I(X) ] = \\sum_{x \\in X} (I(x) \\cdot P(I(X)))$$\n",
    "\n",
    "$$H(X) = \\mathbb{E}[ I(X) ] = -\\sum_{x \\in X} (\\log(P(x)) \\cdot P(x))$$\n",
    "\n",
    "Temos então que quanto maior a entropia menor a previsibilidade ($H = \\text{Previsibilidade}^{-1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc9969-153c-4177-a3fa-76a3097871ad",
   "metadata": {},
   "source": [
    "#### Entropia Conjunta\n",
    "\n",
    "$$H(X,Y) = -\\sum_{x \\in X} \\sum_{y \\in Y} P(x,y) \\cdot \\log (P(x,y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b6227-86c4-4828-81b5-a1b1c2aced2a",
   "metadata": {},
   "source": [
    "#### Entropia Condicional\n",
    "\n",
    "$$H(X|Y) = H(X,Y) - H(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ce8a1-b878-4a11-99c9-8484b4a6747b",
   "metadata": {},
   "source": [
    "#### Informação Mútua\n",
    "\n",
    "É uma medida de dependência mútua entre duas variáveis\n",
    "\n",
    "$$I(X,Y) = H(X) + H(Y) - H(X, Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0150000-bdbb-4b40-b98e-67d933b12cc9",
   "metadata": {},
   "source": [
    "## Teroria dos Grafos\n",
    "\n",
    "$$G = \\{ V,E \\}$$\n",
    "\n",
    "Onde\n",
    "\n",
    "$V$ é um conjunto de vértices\n",
    "\n",
    "$$V = \\{a_1, a_2, a_3, ..., a_n\\}$$ \n",
    "\n",
    "$$n = |V|$$\n",
    "\n",
    "$E$ é um conjunto de arestas\n",
    "\n",
    "$$E = \\{ (a_i, a_j) | a_i, a_j \\in V\\}$$ \n",
    "\n",
    "$$m = |E|$$\n",
    "\n",
    "![](grafos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d5c2a-b48d-4f05-9514-8cb4a9f00fdd",
   "metadata": {},
   "source": [
    "### Tipos de Gráfos\n",
    "\n",
    "#### Gráfos Simples\n",
    "\n",
    "Gráfos onde as arestas são simétricas.\n",
    "\n",
    "$$(a_i, a_j) = (a_j, a_i)$$\n",
    "\n",
    "#### Gráfos Direcionados (Dígrafos)\n",
    "\n",
    "Gráfos onde as arestas são assimétricas.\n",
    "\n",
    "$$(a_i, a_j) \\neq (a_j, a_i)$$\n",
    "\n",
    "#### Gráfos Cíclicos\n",
    "\n",
    "Gráfos onde existe mais de um caminho para se chegar ao mesmo nó.\n",
    "\n",
    "#### Gráfos Acíclicos\n",
    "\n",
    "Gráfos onde existe apenas um caminho para se chegar ao mesmo nó.\n",
    "\n",
    "![](tipos-grafos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478216be-198a-4ee4-af66-e690a595d400",
   "metadata": {},
   "source": [
    "### Árvores\n",
    "\n",
    "Uma Árvore é um **Grafo Direcionado Acíclico (GDA)** onde:\n",
    "\n",
    "- Cada nó só pode receber uma conexão (**Nó Pai**)\n",
    "- Cada nó pode apontar para vários outros (**Nós Filhos**)\n",
    "- É chamado **nó raíz** um nó que não recebe conexões, mas aponta para vários nós (Nós Filhos)\n",
    "- É chamado **nó folha** um nó que não possui nós filhos\n",
    "- Só existe um único caminho entre o nó filho e qualquer nó folha\n",
    "\n",
    "![](arvore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b68916-9a3b-49aa-8f00-44f097567a25",
   "metadata": {},
   "source": [
    "### Árvores de Decisão e Regressão\n",
    "\n",
    "Árvore de Decisão e Regressão é um **método de inferêcia** ($f$) que utiliza modelos na forma de árvores (caixa branca). Os métodos de aprendizagem ($F$) mais utilizados para criar estes modelos utilizam Teoria da Informação para criar os modelos de árvores.\n",
    "\n",
    "##### Modelo de Árvore\n",
    "\n",
    "- Cada nó da árvore é uma pergunta sobre o valor de um atributo\n",
    "- Cada ramo (nó filho) é uma possível resposta\n",
    "- Os nós-folhas são a decisão final \n",
    "  - Uma classe (Árvore de Decisão)\n",
    "  - Um valor (Árvore de Regressão)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9b94a-b707-444d-a211-e6f22491894e",
   "metadata": {},
   "source": [
    "### Árvores de Decisão\n",
    "\n",
    "Cada vértice é uma pergunta sobre um atributo e cada aresta é um dos valores daquele atributo.\n",
    "\n",
    "$$\\mathcal{M}: X \\rightarrow Y$$\n",
    "\n",
    "$$\\mathcal{M} \\text{ é uma árvore (DAG)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a4089-6b3b-419c-8256-1d3d6e1764ab",
   "metadata": {},
   "source": [
    "#### ID3\n",
    "\n",
    "1. Escolha o atributo $\\mathcal{A_i} \\in X$ que melhor separa as instâncias de Y;\n",
    "2. Retire o atributo $\\mathcal{A_i}$ de $X$;\n",
    "3. Para cada valor $v \\in \\mathcal{A_i}$:\n",
    "   1. Crie um nó filho;\n",
    "   2. Selecione o nó filho $\\mathcal{D}_{\\mathcal{A}_i = v}$ das instâncias de $\\mathcal{D}$ tal que $\\mathcal{A}_i = v$;\n",
    "   3. Execute recursivamente a partir do passo 1 usando $\\mathcal{D}_{\\mathcal{A}_i = v}$;\n",
    "4. O algoritmo para quando $X$ estiver vazio;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121c78c-79b9-4e8f-8bc7-b568a8ada7bc",
   "metadata": {},
   "source": [
    "#### Particionamento Recursivo\n",
    "\n",
    "- Particionamento($\\text{Nó}$, $D$):\n",
    "    1. Escolhe um atributo $\\mathcal{A}_i \\in X$ que melhor \"divide\" Y;\n",
    "    2. Cria um $\\text{Nó-Filho}$ em $\\text{Nó}$ para cada valor de $v \\in \\mathcal{A}_i$;\n",
    "- Recursivo($\\text{Nó}$, $D$):\n",
    "  - Para cada $\\text{Nó-Filho}$\n",
    "    1. Selecione o subconjunto $\\mathcal{D}_{\\mathcal{A}_i = v}$ das instâncias de $\\mathcal{D}$ tal que $\\mathcal{A}_i = v$;\n",
    "    2. Chame a etapa de particionamento usando $\\mathcal{D}_{\\mathcal{A}_i = v}$;\n",
    "- O algoritmo termina quando todos os $\\mathcal{A}_i \\in X$ já tiverem sido utilizados;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e9811-066a-4932-934f-5ab1e6936251",
   "metadata": {},
   "source": [
    "#### Algoritmo ID3\n",
    "\n",
    "**função** $\\text{ID3}(\\text{Nó}_{\\text{pai}}, \\mathcal{D}, X, Y ):$  \n",
    "    $\\mathcal{A}_{\\text{escolhido}} \\leftarrow \\arg \\max_{\\mathcal{A}} \\{ \\text{GanhoInformação}(Y, \\mathcal{A}) \\forall \\mathcal{A}_i \\in X\\}$  \n",
    "    $X \\leftarrow X - \\mathcal{A}_{\\text{escolhido}}$  \n",
    "    $\\forall v \\in \\mathcal{A}_{\\text{escolhido}}$  \n",
    "        $\\text{Nó}_{\\text{filho}} \\leftarrow \\text{{}}$  \n",
    "        $\\mathcal{D}_{\\text{filtrado}} \\leftarrow \\mathcal{D} | \\mathcal{A}_{\\text{escolhid}} = v$  \n",
    "        $\\text{Nó}_{\\text{pai}} \\leftarrow \\text{ID3}(\\text{Nó}_{\\text{filho}}, \\mathcal{D}_{\\text{filtrado}}, X, Y)$  \n",
    "    $\\text{retorna} \\text{Nó}_{\\text{pai}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb408f-c1d7-4476-b3dc-30dc87472306",
   "metadata": {},
   "source": [
    "#### Critérios de particionamento\n",
    "\n",
    "Alguns critérios de particionamento são:\n",
    "- Entropia / Ganho de Informação\n",
    "- Índice GINI\n",
    "- Variância Conjunta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095130ee-3d40-459d-96c2-3d9d16117d83",
   "metadata": {},
   "source": [
    "#### Ganho de Informação\n",
    "\n",
    "O Ganho de Informaçõ é uma função baseada na entropia do atributo alvo Y e na entropia dos atributos explicativos X, ele nos diz o quão \"explicativo\" é um atributo X em relação ao atributo Y.\n",
    "\n",
    "Quanto maior o $\\text{GanhoInformação(Y,X)}$, mais o atributo $X$ pode ser utilizado para explicar $Y$.\n",
    "\n",
    "$$\\text{GanhoInformação}(Y,X) = H(Y) - \\sum_{x \\in X} P(x) \\cdot H(Y|x)$$\n",
    "\n",
    "Onde:\n",
    "- $H(Y)$: Entropia de Y, considerando todas as intâncias de Y;\n",
    "- $P(X)$: Probabilidade de $x \\in X$;\n",
    "- $H(Y|X)$: Entropia condicional de Y, considerando somente as instâncias de $Y$ onde $X=x$.\n",
    "\n",
    "##### Razão de Ganho\n",
    "\n",
    "Quanto maior o número de valores de um atributo maior o ganho de informação. Isso nem sempre é positivo e precisamos de uma forma de atingir um equilíbrio para esses valores, Dessa forma normalizamos o ganho de informação pela entropia do atributo.\n",
    "\n",
    "$$\\text{RazãoGanho}(Y,X) = {{\\text{GanhoInformação}(Y,X)} \\over {H(X)}}$$\n",
    "\n",
    "Deve-se então escolher o atributo $\\mathcal{A}_i \\in X$ que maximize a Razão de Ganho, tal que\n",
    "\n",
    "$$\\mathcal{A} = \\arg \\max_{\\mathcal{A}_i} \\{ \\text{RazãoGanho}(Y,X) \\} \\forall \\mathcal{A}_i \\in X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5e081-39e9-4b6e-9b16-0629b8a571e8",
   "metadata": {},
   "source": [
    "#### Valores Numéricos\n",
    "\n",
    "- Divide o domínio em sub-intervalos\n",
    "- Como escolher o melhor ponto de corte?\n",
    "  - Testar o ganho de informação para diferentes valores do domínio dos dados e escolher o melhor ponto de corte\n",
    "- Cria um nó filho para valores menores que o ponto de corte\n",
    "- Cria um nó filho para valores maiores que o ponto de corte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57d2a2-28a6-40ad-90a7-7a26017460d0",
   "metadata": {},
   "source": [
    "## Algoritmo de treinamento de Árvores\n",
    "\n",
    "- *ID3*\n",
    "- *C4.5*\n",
    "- *J48*\n",
    "- *CART*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc075c4e-16d6-4c85-98fd-74374de2c982",
   "metadata": {},
   "source": [
    "### ID3\n",
    "\n",
    "- A geração da árvore depende do atributo-alvo $Y$\n",
    "- O algoritmo é caro computacionalmente\n",
    "  - Percorre todos os atributos\n",
    "  - Calcula o Ganho de Informação para cada valor possível de cada atributo\n",
    "  - Cada ramo da árvore executa recursivamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dfcd1-a3fb-41b3-8295-53063e49b367",
   "metadata": {},
   "source": [
    "### Modelos de Árvores\n",
    "\n",
    "- Especificidade vs. Generalidade\n",
    "  - Excesso de Especificidade = Overfit/Sobre-ajuste\n",
    "    - A árvore tem ramos de mais\n",
    "    - Há ramos na árvore que cobrem muito poucas instâncias (casos muito específicos)\n",
    "  - Excesso de Generalidade = Underfit/Sub-ajuste\n",
    "    - A árvore tem ramos de menos\n",
    "    - Um ramo da árvore cobre instâncias demais (generelização grosseira)\n",
    "- Otimização do modelo\n",
    "  - Minimizar o tamanho da árvore\n",
    "    - Parcimônia = O modelo mais simples é o melhor\n",
    "  - Maximizar o poder de classificação\n",
    "    - Acurácia = O modelo mais preciso é o melhor\n",
    "- Durante o treinamento\n",
    "- Pós treinamento\n",
    "  - Prunning (poda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a19469-eee1-4b13-8ee9-9e1c46d36c61",
   "metadata": {},
   "source": [
    "#### Regras de Produção\n",
    "\n",
    "- Expressa o conhecimento sobre um determinado domínio como um conjunto de regras no formato\n",
    "\n",
    "$$\\text{SE <condição> ENTÃO <ação>}$$\n",
    "\n",
    "- É possível converter uma árvore de decisão em um conjunto de regras de produção\n",
    "  - Cada possível caminho da árvore é uma regra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bef060-12e6-45b5-aedb-91596a0fbdac",
   "metadata": {},
   "source": [
    "## Ensamble Learning\n",
    "\n",
    "### Limitações dos modelos de Árvores\n",
    "\n",
    "- Quando os dados têm alta dimensionalidade\n",
    "  - Centenas de Atributos\n",
    "  - Muitos milhares de Milhões de Instâncias\n",
    "- Um modelo de árvore sozinho\n",
    "  - Ou a árvore ficará gigante\n",
    "  - Ou a árvore ficará muito imprecisa\n",
    "  \n",
    "### Métodos de Ensamble\n",
    "\n",
    "- Ensamble = Comitê\n",
    "  - Ao invés de ter 1 (um) modelo decidindo sozinho, termos muitos decidindo em conjunto\n",
    "  - Cada modelo do comitê tem pontos fortes e fracos, nenhum deles será bom em todas as situações\n",
    "  - Mas uns cobrem as falhas dos outros e, no geral, o Ensemble é mais preciso do que qualquer um dos seus componentes isolados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f66d1-dd08-47a2-bb2c-74a53545e9fb",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "$$\\mathcal{M}: X \\rightarrow Y$$\n",
    "$$\\mathcal{M} = \\{\\mathcal{M}_0, ..., \\mathcal{M}_i\\}$$\n",
    "$$Y = \\cup_{\\mathcal{M}_i \\in \\mathcal{M}} \\mathcal{M}_i(x)$$\n",
    "\n",
    "- Parâmetros\n",
    "  - Quantos modelos?\n",
    "  - Os hiperparâmetros de cada modelo individual $\\mathcal{M}_i$\n",
    "  \n",
    "#### Principais técnicas\n",
    "\n",
    "- Baggin\n",
    "  - Random Forests\n",
    "- Boosting\n",
    "  - Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47165699-f56b-48a2-8c69-fba26a491d1f",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "#### Método de treinamento\n",
    "\n",
    "1. Gerar $M$ novos conjuntos de treinamentos $\\mathcal{D}_i \\subset \\mathcal{D}$ de tamanho $k$\n",
    "   - Cada $\\mathcal{D}_i$ é criado por amostragem uniforme com reposição de $\\mathcal{D}$\n",
    "2. Para cada $\\mathcal{D}_i$, treinar um modelo $\\mathcal{M}_i$ e adicionar no conjunto $\\mathcal{M}$\n",
    "3. Retornar $\\mathcal{M}$\n",
    "\n",
    "#### Método de inferência\n",
    "\n",
    "1. Se o problema for de regressão\n",
    "   - Média: $Y = {1 \\over m} \\sum_{\\mathcal{M}_i \\in \\mathcal{M}} {\\mathcal{M}_i(X)}$\n",
    "2. Se o problema for de classificação\n",
    "   - Votação: $Y = \\arg \\max_\\{Y_j\\}$ onde $Y_j = \\mathcal{M}_i(X), \\forall \\mathcal{M}_i \\in \\mathcal{M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d09a3-059d-43a1-848c-9eb7b77302c7",
   "metadata": {},
   "source": [
    "#### Florestas aleatórias\n",
    "\n",
    "- É um modelo de Bagging utilizando árvores de decisão\n",
    "- A diferença do algoritmo normal é que também é realizado um bagging nos atributos (feature bagging)\n",
    "- Gerar $m$ novos conjuntos de treinamentos $\\mathcal{D}_i \\subset \\mathcal{D}$ com $k$ instâncias e $\\sqrt p$ atributos\n",
    "  - Escolha um subconjunto $\\mathcal{A}_i$ de tamanho $\\sqrt p$ dos atributos $X$\n",
    "  - $\\mathcal{D}_i$ é criado amostrando $k$ instâncias $\\mathcal{D}$ e incluindo somente os atributos do conjunto $\\mathcal{A}_i$\n",
    "  \n",
    "- Prâmetros\n",
    "  - Número de modelos internos\n",
    "  - Tamanho da partição das instâncias\n",
    "  - Tamanho da partição dos atributos\n",
    "  - Parâmetros das árvores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8094b55-2fda-4523-80af-91b81a412f22",
   "metadata": {},
   "source": [
    "[<< Tópico Anterior](04-aprendizado-estatistico.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
